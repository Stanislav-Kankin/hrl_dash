import asyncio
import aiosqlite
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging

logger = logging.getLogger(__name__)


class DataWarehouseService:
    def __init__(self, bitrix_service):
        self.bitrix_service = bitrix_service
        self.db_path = "app/data/warehouse.db"
        self.is_syncing = False
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        import os
        os.makedirs("app/data", exist_ok=True)
        
        async with aiosqlite.connect(self.db_path) as db:
            # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
            await db.execute('''
                CREATE TABLE IF NOT EXISTS activity_snapshots (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    date TEXT NOT NULL,
                    calls INTEGER DEFAULT 0,
                    comments INTEGER DEFAULT 0,
                    tasks INTEGER DEFAULT 0,
                    meetings INTEGER DEFAULT 0,
                    total INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, date)
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
            await db.execute('''
                CREATE TABLE IF NOT EXISTS activities_cache (
                    id INTEGER PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    created TEXT NOT NULL,
                    type_id TEXT NOT NULL,
                    description TEXT,
                    subject TEXT,
                    raw_data TEXT,
                    cached_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    data_date TEXT NOT NULL  -- –î–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏)
                )
            ''')
            
            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            await db.execute('CREATE INDEX IF NOT EXISTS idx_activities_user_date ON activities_cache(user_id, created)')
            await db.execute('CREATE INDEX IF NOT EXISTS idx_snapshots_user_date ON activity_snapshots(user_id, date)')
            await db.execute('CREATE INDEX IF NOT EXISTS idx_activities_data_date ON activities_cache(data_date)')
            
            await db.commit()
        logger.info("‚úÖ Data warehouse initialized")
    
    async def cache_activities(self, activities: List[Dict]):
        """–ö—ç—à–∏—Ä—É–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –ë–î"""
        if not activities:
            return
            
        try:
            async with aiosqlite.connect(self.db_path) as db:
                for activity in activities:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ CREATED –¥–ª—è data_date
                    created_str = activity.get('CREATED', '')
                    try:
                        activity_date = datetime.fromisoformat(created_str.replace('Z', '+00:00'))
                        data_date = activity_date.strftime("%Y-%m-%d")
                    except:
                        data_date = datetime.now().strftime("%Y-%m-%d")
                    
                    await db.execute(
                        '''INSERT OR REPLACE INTO activities_cache 
                           (id, user_id, created, type_id, description, subject, raw_data, data_date)
                           VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                        (
                            activity.get('ID'),
                            activity.get('AUTHOR_ID'),
                            created_str,
                            activity.get('TYPE_ID'),
                            activity.get('DESCRIPTION', ''),
                            activity.get('SUBJECT', ''),
                            json.dumps(activity),
                            data_date
                        )
                    )
                await db.commit()
            logger.info(f"‚úÖ Cached {len(activities)} activities")
        except Exception as e:
            logger.error(f"Error caching activities: {e}")
    
    async def get_cached_activities(self, user_ids: List[str], start_date: str, end_date: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–∑ –∫—ç—à–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in user_ids)
                
                # üî• –ü–†–û–í–ï–†–Ø–ï–ú –ü–û–õ–ù–û–¢–£ –î–ê–ù–ù–´–• –ó–ê –ü–ï–†–ò–û–î
                completeness_check = '''
                    SELECT COUNT(DISTINCT data_date) as cached_days
                    FROM activities_cache 
                    WHERE user_id IN ({}) 
                    AND data_date BETWEEN ? AND ?
                '''.format(placeholders)
                
                params = user_ids + [start_date, end_date]
                cursor = await db.execute(completeness_check, params)
                result = await cursor.fetchone()
                
                if not result:
                    return []
                
                cached_days = result[0]
                
                # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –≤ –ø–µ—Ä–∏–æ–¥–µ
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                # üî• –°—á–∏—Ç–∞–µ–º –∫—ç—à –≤–∞–ª–∏–¥–Ω—ã–º –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ –í–°–ï –¥–Ω–∏ –ø–µ—Ä–∏–æ–¥–∞
                if cached_days < total_days:
                    logger.info(f"üîÑ Cache incomplete: {cached_days}/{total_days} days, will refresh")
                    return []
                
                # üî• –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω—ã–µ - –ø–æ–ª—É—á–∞–µ–º –∏—Ö (–ë–ï–ó –ü–†–û–í–ï–†–ö–ò –í–†–ï–ú–ï–ù–ò –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø)
                query = f'''
                    SELECT raw_data FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                    ORDER BY created DESC
                '''
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                activities = []
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        activities.append(activity_data)
                    except Exception as e:
                        logger.error(f"Error parsing cached activity: {e}")
                        continue
                        
                logger.info(f"üìä Got {len(activities)} activities from cache (complete period: {start_date} to {end_date})")
                return activities
                
        except Exception as e:
            logger.error(f"Error getting cached activities: {e}")
            return []
    
    async def save_daily_snapshot(self, user_stats: List[Dict], date: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                for stat in user_stats:
                    await db.execute(
                        '''INSERT OR REPLACE INTO activity_snapshots 
                           (user_id, date, calls, comments, tasks, meetings, total)
                           VALUES (?, ?, ?, ?, ?, ?, ?)''',
                        (
                            stat['user_id'],
                            date,
                            stat.get('calls', 0),
                            stat.get('comments', 0),
                            stat.get('tasks', 0),
                            stat.get('meetings', 0),
                            stat.get('total', 0)
                        )
                    )
                await db.commit()
            logger.info(f"‚úÖ Saved daily snapshot for {date}")
        except Exception as e:
            logger.error(f"Error saving daily snapshot: {e}")
    
    async def get_fast_stats(self, user_ids: List[str], start_date: str, end_date: str) -> Optional[Dict]:
        """–ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ –∫—ç—à–∞ –±–µ–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Bitrix"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # –ü–æ–ª—É—á–∞–µ–º —Å–Ω–∞–ø—à–æ—Ç—ã –∑–∞ –ø–µ—Ä–∏–æ–¥
                query = '''
                    SELECT user_id, date, calls, comments, tasks, meetings, total 
                    FROM activity_snapshots 
                    WHERE user_id IN ({}) AND date BETWEEN ? AND ?
                    ORDER BY date DESC
                '''.format(','.join('?' * len(user_ids)))
                
                params = user_ids + [start_date, end_date]
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                if not rows:
                    return None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
                unique_dates = set(row[1] for row in rows)
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                if len(unique_dates) < total_days:
                    logger.info(f"üìä Fast stats incomplete: {len(unique_dates)}/{total_days} days")
                    return None
                
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
                user_stats = {}
                for row in rows:
                    user_id = row[0]
                    if user_id not in user_stats:
                        user_stats[user_id] = {
                            'user_id': user_id,
                            'calls': 0, 'comments': 0, 'tasks': 0, 
                            'meetings': 0, 'total': 0, 'days_count': set()
                        }
                    
                    user_stats[user_id]['calls'] += row[2]
                    user_stats[user_id]['comments'] += row[3]
                    user_stats[user_id]['tasks'] += row[4]
                    user_stats[user_id]['meetings'] += row[5]
                    user_stats[user_id]['total'] += row[6]
                    user_stats[user_id]['days_count'].add(row[1])
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º days_count –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                for stat in user_stats.values():
                    stat['days_count'] = len(stat['days_count'])
                
                return {
                    'user_stats': list(user_stats.values()),
                    'total_activities': sum(stat['total'] for stat in user_stats.values()),
                    'from_cache': True,
                    'cache_date': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Error getting fast stats: {e}")
            return None

    async def is_period_cached(self, user_ids: List[str], start_date: str, end_date: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥ –≤ –∫—ç—à–µ"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–Ω–µ–π –ø–µ—Ä–∏–æ–¥–∞
                query = '''
                    SELECT COUNT(DISTINCT date) as days_count
                    FROM activity_snapshots 
                    WHERE user_id IN ({}) AND date BETWEEN ? AND ?
                '''.format(','.join('?' * len(user_ids)))
                
                params = user_ids + [start_date, end_date]
                cursor = await db.execute(query, params)
                result = await cursor.fetchone()
                
                if not result:
                    return False
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –≤ –ø–µ—Ä–∏–æ–¥–µ
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                # –°—á–∏—Ç–∞–µ–º –∫—ç—à –ø–æ–ª–Ω—ã–º –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ –í–°–ï –¥–Ω–∏
                return result[0] >= total_days
                
        except Exception as e:
            logger.error(f"Error checking cache completeness: {e}")
            return False

    async def start_background_sync(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ - –û–¢–ö–õ–Æ–ß–ï–ù–ê"""
        logger.info("üîÑ Background sync DISABLED - caching only on user requests")
        return
    
    async def sync_recent_data(self):
        """–§–æ–Ω–æ–≤–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è - –û–¢–ö–õ–Æ–ß–ï–ù–ê"""
        return

    async def save_daily_snapshot_from_activities(self, activities: List[Dict], user_ids: List[str], date: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π"""
        if not activities:
            return
            
        try:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
            user_activities = {}
            for act in activities:
                user_id = str(act.get('AUTHOR_ID', ''))
                if user_id in user_ids:
                    if user_id not in user_activities:
                        user_activities[user_id] = []
                    user_activities[user_id].append(act)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_stats = []
            for user_id, acts in user_activities.items():
                calls = len([a for a in acts if str(a.get('TYPE_ID')) == '2'])
                comments = len([a for a in acts if str(a.get('TYPE_ID')) == '6'])
                tasks = len([a for a in acts if str(a.get('TYPE_ID')) == '4'])
                meetings = len([a for a in acts if str(a.get('TYPE_ID')) == '1'])
                total = len(acts)
                
                user_stats.append({
                    "user_id": user_id,
                    "calls": calls,
                    "comments": comments,
                    "tasks": tasks,
                    "meetings": meetings,
                    "total": total
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            await self.save_daily_snapshot(user_stats, date)
            
        except Exception as e:
            logger.error(f"Error saving snapshot from activities: {e}")

    async def clear_old_cache(self, days_to_keep: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–π –∫—ç—à"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).strftime("%Y-%m-%d")
            
            async with aiosqlite.connect(self.db_path) as db:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
                await db.execute(
                    "DELETE FROM activities_cache WHERE data_date < ?",
                    (cutoff_date,)
                )
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–Ω–∞–ø—à–æ—Ç—ã
                await db.execute(
                    "DELETE FROM activity_snapshots WHERE date < ?",
                    (cutoff_date,)
                )
                await db.commit()
                
                logger.info(f"üßπ Cleared cache older than {cutoff_date}")
        except Exception as e:
            logger.error(f"Error clearing old cache: {e}")
    
    async def get_cached_activities_direct(self, user_ids: List[str], start_date: str, end_date: str, activity_types: List[str] = None) -> Dict:
        """
        üî• –°–£–ü–ï–†-–ü–†–û–°–¢–û–ô –ú–ï–¢–û–î - —Å—á–∏—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç—É —Ç–æ–ª—å–∫–æ –ø–æ –†–ê–ë–û–ß–ò–ú –¥–Ω—è–º
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in user_ids)
                
                query = f'''
                    SELECT raw_data, data_date FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                '''
                params = user_ids + [start_date, end_date]
                
                if activity_types and activity_types != ['all']:
                    type_placeholders = ','.join('?' for _ in activity_types)
                    query += f' AND type_id IN ({type_placeholders})'
                    params.extend(activity_types)
                
                query += ' ORDER BY created DESC'
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                activities = []
                cached_dates = set()
                
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        activities.append(activity_data)
                        cached_dates.add(row[1])
                    except Exception as e:
                        continue
                
                # üî• –°–ß–ò–¢–ê–ï–ú –¢–û–õ–¨–ö–û –†–ê–ë–û–ß–ò–ï –î–ù–ò
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                
                work_days = 0
                current = start
                while current <= end:
                    # –ü–Ω=0, –í—Ç=1, –°—Ä=2, –ß—Ç=3, –ü—Ç=4, –°–±=5, –í—Å=6
                    if current.weekday() < 5:  # –¢–æ–ª—å–∫–æ –ø–Ω-–ø—Ç
                        work_days += 1
                    current += timedelta(days=1)
                
                # –°—á–∏—Ç–∞–µ–º —Ä–∞–±–æ—á–∏–µ –¥–Ω–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
                work_days_with_data = 0
                current = start
                while current <= end:
                    date_str = current.strftime("%Y-%m-%d")
                    if current.weekday() < 5 and date_str in cached_dates:
                        work_days_with_data += 1
                    current += timedelta(days=1)
                
                completeness = (work_days_with_data / work_days) * 100 if work_days > 0 else 0
                
                logger.info(f"üöÄ Direct cache access: {len(activities)} activities, {work_days_with_data}/{work_days} work days ({completeness:.1f}% complete)")
                
                return {
                    "activities": activities,
                    "completeness": completeness,
                    "work_days_with_data": work_days_with_data,
                    "total_work_days": work_days,
                    "note": "–ü–æ–ª–Ω–æ—Ç–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞–±–æ—á–∏–º –¥–Ω—è–º (–ø–Ω-–ø—Ç)"
                }
                    
        except Exception as e:
            logger.error(f"Error in direct cache access: {e}")
            return {"activities": [], "completeness": 0}
        
    async def get_cached_activities_optimized(self, user_ids: List[str], start_date: str, end_date: str, activity_types: List[str] = None) -> Dict:
        """
        –£–º–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —Ç–∏–ø—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in user_ids)
                
                # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                query = f'''
                    SELECT raw_data, data_date FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                '''
                params = user_ids + [start_date, end_date]
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if activity_types and activity_types != ['all']:
                    type_placeholders = ','.join('?' for _ in activity_types)
                    query += f' AND type_id IN ({type_placeholders})'
                    params.extend(activity_types)
                
                query += ' ORDER BY created DESC'
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                if not rows:
                    return {"activities": [], "missing_days": [], "completeness": 0}
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                cached_dates = set()
                activities = []
                
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        activities.append(activity_data)
                        cached_dates.add(row[1])
                    except Exception as e:
                        continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–Ω–∏
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                missing_days = []
                current = start
                while current <= end:
                    date_str = current.strftime("%Y-%m-%d")
                    if date_str not in cached_dates:
                        missing_days.append(date_str)
                    current += timedelta(days=1)
                
                completeness = ((total_days - len(missing_days)) / total_days) * 100
                
                logger.info(f"üìä Cache analysis: {len(activities)} activities, {completeness:.1f}% complete ({total_days - len(missing_days)}/{total_days} days)")
                
                if missing_days:
                    logger.info(f"üîÑ Missing days: {missing_days}")
                
                return {
                    "activities": activities,
                    "missing_days": missing_days,
                    "completeness": completeness,
                    "cached_days_count": len(cached_dates),
                    "total_days": total_days
                }
                    
        except Exception as e:
            logger.error(f"Error analyzing cache: {e}")
            return {"activities": [], "missing_days": [], "completeness": 0}
        
    async def get_cached_activities_for_selected_users(self, selected_user_ids: List[str], start_date: str, end_date: str, activity_types: List[str] = None) -> Dict:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –∫—ç—à–∞ –¢–û–õ–¨–ö–û –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        —Å —É–º–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è —Ä–∞–∑–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in selected_user_ids)
                
                # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                query = f'''
                    SELECT raw_data, data_date FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                '''
                params = selected_user_ids + [start_date, end_date]
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                if activity_types and activity_types != ['all']:
                    type_placeholders = ','.join('?' for _ in activity_types)
                    query += f' AND type_id IN ({type_placeholders})'
                    params.extend(activity_types)
                
                query += ' ORDER BY created DESC'
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                if not rows:
                    return {"activities": [], "missing_days": [], "completeness": 0, "selected_users": selected_user_ids}
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¢–û–õ–¨–ö–û –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                activities = []
                user_activities = {user_id: [] for user_id in selected_user_ids}
                all_dates = set()
                
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        user_id = str(activity_data.get('AUTHOR_ID'))
                        if user_id in selected_user_ids:
                            activities.append(activity_data)
                            user_activities[user_id].append(activity_data)
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ CREATED
                            created_str = activity_data.get('CREATED', '').replace('Z', '+00:00')
                            activity_date = datetime.fromisoformat(created_str).strftime('%Y-%m-%d')
                            all_dates.add(activity_date)
                    except Exception as e:
                        continue
                
                # üî• –£–ú–ù–ê–Ø –õ–û–ì–ò–ö–ê: —Ä–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Ä–∞–∑–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                # –°—á–∏—Ç–∞–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–Ω–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                user_days_coverage = {}
                for user_id, user_acts in user_activities.items():
                    user_dates = set()
                    for act in user_acts:
                        try:
                            created_str = act.get('CREATED', '').replace('Z', '+00:00')
                            activity_date = datetime.fromisoformat(created_str).strftime('%Y-%m-%d')
                            user_dates.add(activity_date)
                        except:
                            continue
                    user_days_coverage[user_id] = user_dates
                
                # üî• –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —Ä–∞–∑–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                if len(selected_user_ids) == 1:
                    # –î–ª—è –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: —Ç—Ä–µ–±—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –∑–∞ —Ä–∞–±–æ—á–∏–µ –¥–Ω–∏
                    user_id = selected_user_ids[0]
                    user_dates = user_days_coverage.get(user_id, set())
                    
                    # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ –¥–Ω–∏ (–ø–Ω-–ø—Ç)
                    work_days = 0
                    current = start
                    while current <= end:
                        # –ü–Ω=0, –í—Ç=1, –°—Ä=2, –ß—Ç=3, –ü—Ç=4, –°–±=5, –í—Å=6
                        if current.weekday() < 5:  # –¢–æ–ª—å–∫–æ –ø–Ω-–ø—Ç
                            work_days += 1
                        current += timedelta(days=1)
                    
                    # –î–ª—è –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—á–∏—Ç–∞–µ–º –ø–æ–ª–Ω–æ—Ç—É —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞–±–æ—á–∏–º –¥–Ω—è–º
                    user_work_days_with_data = 0
                    current = start
                    while current <= end:
                        date_str = current.strftime("%Y-%m-%d")
                        if current.weekday() < 5 and date_str in user_dates:  # –†–∞–±–æ—á–∏–π –¥–µ–Ω—å —Å –¥–∞–Ω–Ω—ã–º–∏
                            user_work_days_with_data += 1
                        current += timedelta(days=1)
                    
                    completeness = (user_work_days_with_data / work_days) * 100 if work_days > 0 else 0
                    missing_days = []
                    
                    logger.info(f"üìä Single user cache: {user_work_days_with_data}/{work_days} work days ({completeness:.1f}%)")
                    
                else:
                    # –î–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ
                    all_covered_days = set()
                    for user_dates in user_days_coverage.values():
                        all_covered_days.update(user_dates)
                    
                    missing_days = []
                    current = start
                    while current <= end:
                        date_str = current.strftime("%Y-%m-%d")
                        if date_str not in all_covered_days:
                            missing_days.append(date_str)
                        current += timedelta(days=1)
                    
                    completeness = ((total_days - len(missing_days)) / total_days) * 100
                
                # üî• –ê–î–ê–ü–¢–ò–í–ù–´–ï –ü–û–†–û–ì–ò –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                user_coverage_info = {}
                for user_id in selected_user_ids:
                    user_dates = user_days_coverage.get(user_id, set())
                    user_coverage_info[user_id] = {
                        'days_with_data': len(user_dates),
                        'total_days': total_days,
                        'coverage_percent': (len(user_dates) / total_days) * 100 if total_days > 0 else 0
                    }
                
                logger.info(f"üìä Smart cache analysis for {len(selected_user_ids)} users: {len(activities)} activities, {completeness:.1f}% complete")
                
                return {
                    "activities": activities,
                    "missing_days": missing_days,
                    "completeness": completeness,
                    "total_days": total_days,
                    "selected_users": selected_user_ids,
                    "user_coverage_info": user_coverage_info,
                    "total_activities": len(activities),
                    "user_count": len(selected_user_ids)  # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                }
                    
        except Exception as e:
            logger.error(f"Error analyzing cache for selected users: {e}")
            return {"activities": [], "missing_days": [], "completeness": 0, "selected_users": selected_user_ids}

    async def get_cached_activities_simple(self, user_ids: List[str], start_date: str, end_date: str, activity_types: List[str] = None) -> Dict:
        """
        üî• –£–ü–†–û–©–ï–ù–ù–´–ô –ú–ï–¢–û–î –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–∏
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in user_ids)
                
                # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å - –ø–æ–ª—É—á–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
                query = f'''
                    SELECT raw_data, data_date FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                '''
                params = user_ids + [start_date, end_date]
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                if activity_types and activity_types != ['all']:
                    type_placeholders = ','.join('?' for _ in activity_types)
                    query += f' AND type_id IN ({type_placeholders})'
                    params.extend(activity_types)
                
                query += ' ORDER BY created DESC'
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                if not rows:
                    return {"activities": [], "completeness": 0}
                
                # –ü—Ä–æ—Å—Ç–æ —Å–æ–±–∏—Ä–∞–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                activities = []
                cached_dates = set()
                
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        activities.append(activity_data)
                        cached_dates.add(row[1])
                    except Exception as e:
                        continue
                
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã - —Å—á–∏—Ç–∞–µ–º –¥–Ω–∏
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                completeness = (len(cached_dates) / total_days) * 100
                
                logger.info(f"‚ö° Simple cache check: {len(activities)} activities, {completeness:.1f}% complete ({len(cached_dates)}/{total_days} days)")
                
                return {
                    "activities": activities,
                    "completeness": completeness,
                    "cached_days": len(cached_dates),
                    "total_days": total_days
                }
                    
        except Exception as e:
            logger.error(f"Error in simple cache check: {e}")
            return {"activities": [], "completeness": 0}