import asyncio
import aiosqlite
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging

logger = logging.getLogger(__name__)


class DataWarehouseService:
    def __init__(self, bitrix_service):
        self.bitrix_service = bitrix_service
        self.db_path = "app/data/warehouse.db"
        self.is_syncing = False
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        import os
        os.makedirs("app/data", exist_ok=True)
        
        async with aiosqlite.connect(self.db_path) as db:
            # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
            await db.execute('''
                CREATE TABLE IF NOT EXISTS activity_snapshots (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    date TEXT NOT NULL,
                    calls INTEGER DEFAULT 0,
                    comments INTEGER DEFAULT 0,
                    tasks INTEGER DEFAULT 0,
                    meetings INTEGER DEFAULT 0,
                    total INTEGER DEFAULT 0,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, date)
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
            await db.execute('''
                CREATE TABLE IF NOT EXISTS activities_cache (
                    id INTEGER PRIMARY KEY,
                    user_id TEXT NOT NULL,
                    created TEXT NOT NULL,
                    type_id TEXT NOT NULL,
                    description TEXT,
                    subject TEXT,
                    raw_data TEXT,
                    cached_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    data_date TEXT NOT NULL  -- –î–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏)
                )
            ''')
            
            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            await db.execute('CREATE INDEX IF NOT EXISTS idx_activities_user_date ON activities_cache(user_id, created)')
            await db.execute('CREATE INDEX IF NOT EXISTS idx_snapshots_user_date ON activity_snapshots(user_id, date)')
            await db.execute('CREATE INDEX IF NOT EXISTS idx_activities_data_date ON activities_cache(data_date)')
            
            await db.commit()
        logger.info("‚úÖ Data warehouse initialized")
    
    async def cache_activities(self, activities: List[Dict]):
        """–ö—ç—à–∏—Ä—É–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –ë–î"""
        if not activities:
            return
            
        try:
            async with aiosqlite.connect(self.db_path) as db:
                for activity in activities:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ CREATED –¥–ª—è data_date
                    created_str = activity.get('CREATED', '')
                    try:
                        activity_date = datetime.fromisoformat(created_str.replace('Z', '+00:00'))
                        data_date = activity_date.strftime("%Y-%m-%d")
                    except:
                        data_date = datetime.now().strftime("%Y-%m-%d")
                    
                    await db.execute(
                        '''INSERT OR REPLACE INTO activities_cache 
                           (id, user_id, created, type_id, description, subject, raw_data, data_date)
                           VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                        (
                            activity.get('ID'),
                            activity.get('AUTHOR_ID'),
                            created_str,
                            activity.get('TYPE_ID'),
                            activity.get('DESCRIPTION', ''),
                            activity.get('SUBJECT', ''),
                            json.dumps(activity),
                            data_date
                        )
                    )
                await db.commit()
            logger.info(f"‚úÖ Cached {len(activities)} activities")
        except Exception as e:
            logger.error(f"Error caching activities: {e}")
    
    async def get_cached_activities(self, user_ids: List[str], start_date: str, end_date: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–∑ –∫—ç—à–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in user_ids)
                
                # üî• –ü–†–û–í–ï–†–Ø–ï–ú –ü–û–õ–ù–û–¢–£ –î–ê–ù–ù–´–• –ó–ê –ü–ï–†–ò–û–î
                completeness_check = '''
                    SELECT COUNT(DISTINCT data_date) as cached_days
                    FROM activities_cache 
                    WHERE user_id IN ({}) 
                    AND data_date BETWEEN ? AND ?
                '''.format(placeholders)
                
                params = user_ids + [start_date, end_date]
                cursor = await db.execute(completeness_check, params)
                result = await cursor.fetchone()
                
                if not result:
                    return []
                
                cached_days = result[0]
                
                # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –≤ –ø–µ—Ä–∏–æ–¥–µ
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                # üî• –°—á–∏—Ç–∞–µ–º –∫—ç—à –≤–∞–ª–∏–¥–Ω—ã–º –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ –í–°–ï –¥–Ω–∏ –ø–µ—Ä–∏–æ–¥–∞
                if cached_days < total_days:
                    logger.info(f"üîÑ Cache incomplete: {cached_days}/{total_days} days, will refresh")
                    return []
                
                # üî• –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω—ã–µ - –ø–æ–ª—É—á–∞–µ–º –∏—Ö (–ë–ï–ó –ü–†–û–í–ï–†–ö–ò –í–†–ï–ú–ï–ù–ò –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø)
                query = f'''
                    SELECT raw_data FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                    ORDER BY created DESC
                '''
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                activities = []
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        activities.append(activity_data)
                    except Exception as e:
                        logger.error(f"Error parsing cached activity: {e}")
                        continue
                        
                logger.info(f"üìä Got {len(activities)} activities from cache (complete period: {start_date} to {end_date})")
                return activities
                
        except Exception as e:
            logger.error(f"Error getting cached activities: {e}")
            return []
    
    async def save_daily_snapshot(self, user_stats: List[Dict], date: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                for stat in user_stats:
                    await db.execute(
                        '''INSERT OR REPLACE INTO activity_snapshots 
                           (user_id, date, calls, comments, tasks, meetings, total)
                           VALUES (?, ?, ?, ?, ?, ?, ?)''',
                        (
                            stat['user_id'],
                            date,
                            stat.get('calls', 0),
                            stat.get('comments', 0),
                            stat.get('tasks', 0),
                            stat.get('meetings', 0),
                            stat.get('total', 0)
                        )
                    )
                await db.commit()
            logger.info(f"‚úÖ Saved daily snapshot for {date}")
        except Exception as e:
            logger.error(f"Error saving daily snapshot: {e}")
    
    async def get_fast_stats(self, user_ids: List[str], start_date: str, end_date: str) -> Optional[Dict]:
        """–ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ –∫—ç—à–∞ –±–µ–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Bitrix"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # –ü–æ–ª—É—á–∞–µ–º —Å–Ω–∞–ø—à–æ—Ç—ã –∑–∞ –ø–µ—Ä–∏–æ–¥
                query = '''
                    SELECT user_id, date, calls, comments, tasks, meetings, total 
                    FROM activity_snapshots 
                    WHERE user_id IN ({}) AND date BETWEEN ? AND ?
                    ORDER BY date DESC
                '''.format(','.join('?' * len(user_ids)))
                
                params = user_ids + [start_date, end_date]
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                if not rows:
                    return None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
                unique_dates = set(row[1] for row in rows)
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                if len(unique_dates) < total_days:
                    logger.info(f"üìä Fast stats incomplete: {len(unique_dates)}/{total_days} days")
                    return None
                
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
                user_stats = {}
                for row in rows:
                    user_id = row[0]
                    if user_id not in user_stats:
                        user_stats[user_id] = {
                            'user_id': user_id,
                            'calls': 0, 'comments': 0, 'tasks': 0, 
                            'meetings': 0, 'total': 0, 'days_count': set()
                        }
                    
                    user_stats[user_id]['calls'] += row[2]
                    user_stats[user_id]['comments'] += row[3]
                    user_stats[user_id]['tasks'] += row[4]
                    user_stats[user_id]['meetings'] += row[5]
                    user_stats[user_id]['total'] += row[6]
                    user_stats[user_id]['days_count'].add(row[1])
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º days_count –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                for stat in user_stats.values():
                    stat['days_count'] = len(stat['days_count'])
                
                return {
                    'user_stats': list(user_stats.values()),
                    'total_activities': sum(stat['total'] for stat in user_stats.values()),
                    'from_cache': True,
                    'cache_date': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Error getting fast stats: {e}")
            return None

    async def is_period_cached(self, user_ids: List[str], start_date: str, end_date: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥ –≤ –∫—ç—à–µ"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–Ω–µ–π –ø–µ—Ä–∏–æ–¥–∞
                query = '''
                    SELECT COUNT(DISTINCT date) as days_count
                    FROM activity_snapshots 
                    WHERE user_id IN ({}) AND date BETWEEN ? AND ?
                '''.format(','.join('?' * len(user_ids)))
                
                params = user_ids + [start_date, end_date]
                cursor = await db.execute(query, params)
                result = await cursor.fetchone()
                
                if not result:
                    return False
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –≤ –ø–µ—Ä–∏–æ–¥–µ
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                # –°—á–∏—Ç–∞–µ–º –∫—ç—à –ø–æ–ª–Ω—ã–º –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ –í–°–ï –¥–Ω–∏
                return result[0] >= total_days
                
        except Exception as e:
            logger.error(f"Error checking cache completeness: {e}")
            return False

    async def start_background_sync(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ - –û–¢–ö–õ–Æ–ß–ï–ù–ê"""
        logger.info("üîÑ Background sync DISABLED - caching only on user requests")
        return
    
    async def sync_recent_data(self):
        """–§–æ–Ω–æ–≤–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è - –û–¢–ö–õ–Æ–ß–ï–ù–ê"""
        return

    async def save_daily_snapshot_from_activities(self, activities: List[Dict], user_ids: List[str], date: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–Ω–∞–ø—à–æ—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π"""
        if not activities:
            return
            
        try:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
            user_activities = {}
            for act in activities:
                user_id = str(act.get('AUTHOR_ID', ''))
                if user_id in user_ids:
                    if user_id not in user_activities:
                        user_activities[user_id] = []
                    user_activities[user_id].append(act)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_stats = []
            for user_id, acts in user_activities.items():
                calls = len([a for a in acts if str(a.get('TYPE_ID')) == '2'])
                comments = len([a for a in acts if str(a.get('TYPE_ID')) == '6'])
                tasks = len([a for a in acts if str(a.get('TYPE_ID')) == '4'])
                meetings = len([a for a in acts if str(a.get('TYPE_ID')) == '1'])
                total = len(acts)
                
                user_stats.append({
                    "user_id": user_id,
                    "calls": calls,
                    "comments": comments,
                    "tasks": tasks,
                    "meetings": meetings,
                    "total": total
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            await self.save_daily_snapshot(user_stats, date)
            
        except Exception as e:
            logger.error(f"Error saving snapshot from activities: {e}")

    async def clear_old_cache(self, days_to_keep: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–π –∫—ç—à"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).strftime("%Y-%m-%d")
            
            async with aiosqlite.connect(self.db_path) as db:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π
                await db.execute(
                    "DELETE FROM activities_cache WHERE data_date < ?",
                    (cutoff_date,)
                )
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–Ω–∞–ø—à–æ—Ç—ã
                await db.execute(
                    "DELETE FROM activity_snapshots WHERE date < ?",
                    (cutoff_date,)
                )
                await db.commit()
                
                logger.info(f"üßπ Cleared cache older than {cutoff_date}")
        except Exception as e:
            logger.error(f"Error clearing old cache: {e}")
    
    async def get_cached_activities_optimized(self, user_ids: List[str], start_date: str, end_date: str, activity_types: List[str] = None) -> Dict:
        """
        –£–º–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —Ç–∏–ø—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                placeholders = ','.join('?' for _ in user_ids)
                
                # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                query = f'''
                    SELECT raw_data, data_date FROM activities_cache 
                    WHERE user_id IN ({placeholders}) 
                    AND data_date BETWEEN ? AND ?
                '''
                params = user_ids + [start_date, end_date]
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if activity_types and activity_types != ['all']:
                    type_placeholders = ','.join('?' for _ in activity_types)
                    query += f' AND type_id IN ({type_placeholders})'
                    params.extend(activity_types)
                
                query += ' ORDER BY created DESC'
                
                cursor = await db.execute(query, params)
                rows = await cursor.fetchall()
                
                if not rows:
                    return {"activities": [], "missing_days": [], "completeness": 0}
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                cached_dates = set()
                activities = []
                
                for row in rows:
                    try:
                        activity_data = json.loads(row[0])
                        activities.append(activity_data)
                        cached_dates.add(row[1])
                    except Exception as e:
                        continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–Ω–∏
                start = datetime.fromisoformat(start_date)
                end = datetime.fromisoformat(end_date)
                total_days = (end - start).days + 1
                
                missing_days = []
                current = start
                while current <= end:
                    date_str = current.strftime("%Y-%m-%d")
                    if date_str not in cached_dates:
                        missing_days.append(date_str)
                    current += timedelta(days=1)
                
                completeness = ((total_days - len(missing_days)) / total_days) * 100
                
                logger.info(f"üìä Cache analysis: {len(activities)} activities, {completeness:.1f}% complete ({total_days - len(missing_days)}/{total_days} days)")
                
                if missing_days:
                    logger.info(f"üîÑ Missing days: {missing_days}")
                
                return {
                    "activities": activities,
                    "missing_days": missing_days,
                    "completeness": completeness,
                    "cached_days_count": len(cached_dates),
                    "total_days": total_days
                }
                    
        except Exception as e:
            logger.error(f"Error analyzing cache: {e}")
            return {"activities": [], "missing_days": [], "completeness": 0}
                    
        except Exception as e:
            logger.error(f"Error analyzing cache: {e}")
            return {"activities": [], "missing_days": [], "completeness": 0}